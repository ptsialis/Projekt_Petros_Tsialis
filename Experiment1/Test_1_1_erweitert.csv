;name_of_the_features;id_of_the_features;name_of_the_dataset;datatype_of_the_feature;dataset_min_value;dataset_max_value;data_descr;chat_gpt_just_feature;chat_gpt_all_feature;chat_gpt_desc_feature
0;RIDGE;871;pollen;numeric;-23,2839;21,4066;"**Author**:   
**Source**: Unknown - Date unknown  
**Please cite**:   

Binarized version of the original data set (see version 1). It converts the numeric target feature to a two-class nominal target feature by computing the mean and classifying all instances with a lower target value as positive ('P') and all others as negative ('N').";;;
1;NUB;871;pollen;numeric;-16,3935;17,2583;"**Author**:   
**Source**: Unknown - Date unknown  
**Please cite**:   

Binarized version of the original data set (see version 1). It converts the numeric target feature to a two-class nominal target feature by computing the mean and classifying all instances with a lower target value as positive ('P') and all others as negative ('N').";;;
2;CRACK;871;pollen;numeric;-31,413;30,3178;"**Author**:   
**Source**: Unknown - Date unknown  
**Please cite**:   

Binarized version of the original data set (see version 1). It converts the numeric target feature to a two-class nominal target feature by computing the mean and classifying all instances with a lower target value as positive ('P') and all others as negative ('N').";;;
3;WEIGHT;871;pollen;numeric;-34,0352;35,8028;"**Author**:   
**Source**: Unknown - Date unknown  
**Please cite**:   

Binarized version of the original data set (see version 1). It converts the numeric target feature to a two-class nominal target feature by computing the mean and classifying all instances with a lower target value as positive ('P') and all others as negative ('N').";;;
4;DENSITY;871;pollen;numeric;-12,0391;10,8673;"**Author**:   
**Source**: Unknown - Date unknown  
**Please cite**:   

Binarized version of the original data set (see version 1). It converts the numeric target feature to a two-class nominal target feature by computing the mean and classifying all instances with a lower target value as positive ('P') and all others as negative ('N').";;;
5;a1;23515;sulfur;numeric;0;1;"&quot;The sulfur recovery unit (SRU) removes environmental pollutants from acid gas
streams before they are released into the atmosphere. Furthermore, elemental sulfur
is recovered as a valuable by-product.&quot;

5 inputs variables are gas and air flows.
2 outputs to predict are H2S and SO2 concentrations

See Appendix A.5 of Fortuna, L., Graziani, S., Rizzo, A., Xibilia, M.G. &quot;Soft Sensors for Monitoring and Control of Industrial Processes&quot; (Springer 2007) for more info.";;;
6;a2;23515;sulfur;numeric;0;1;"&quot;The sulfur recovery unit (SRU) removes environmental pollutants from acid gas
streams before they are released into the atmosphere. Furthermore, elemental sulfur
is recovered as a valuable by-product.&quot;

5 inputs variables are gas and air flows.
2 outputs to predict are H2S and SO2 concentrations

See Appendix A.5 of Fortuna, L., Graziani, S., Rizzo, A., Xibilia, M.G. &quot;Soft Sensors for Monitoring and Control of Industrial Processes&quot; (Springer 2007) for more info.";;;
7;a3;23515;sulfur;numeric;0;1;"&quot;The sulfur recovery unit (SRU) removes environmental pollutants from acid gas
streams before they are released into the atmosphere. Furthermore, elemental sulfur
is recovered as a valuable by-product.&quot;

5 inputs variables are gas and air flows.
2 outputs to predict are H2S and SO2 concentrations

See Appendix A.5 of Fortuna, L., Graziani, S., Rizzo, A., Xibilia, M.G. &quot;Soft Sensors for Monitoring and Control of Industrial Processes&quot; (Springer 2007) for more info.";;;
8;a4;23515;sulfur;numeric;0;1;"&quot;The sulfur recovery unit (SRU) removes environmental pollutants from acid gas
streams before they are released into the atmosphere. Furthermore, elemental sulfur
is recovered as a valuable by-product.&quot;

5 inputs variables are gas and air flows.
2 outputs to predict are H2S and SO2 concentrations

See Appendix A.5 of Fortuna, L., Graziani, S., Rizzo, A., Xibilia, M.G. &quot;Soft Sensors for Monitoring and Control of Industrial Processes&quot; (Springer 2007) for more info.";;;
9;a5;23515;sulfur;numeric;0;1;"&quot;The sulfur recovery unit (SRU) removes environmental pollutants from acid gas
streams before they are released into the atmosphere. Furthermore, elemental sulfur
is recovered as a valuable by-product.&quot;

5 inputs variables are gas and air flows.
2 outputs to predict are H2S and SO2 concentrations

See Appendix A.5 of Fortuna, L., Graziani, S., Rizzo, A., Xibilia, M.G. &quot;Soft Sensors for Monitoring and Control of Industrial Processes&quot; (Springer 2007) for more info.";;;
10;y1;23515;sulfur;numeric;0;1;"&quot;The sulfur recovery unit (SRU) removes environmental pollutants from acid gas
streams before they are released into the atmosphere. Furthermore, elemental sulfur
is recovered as a valuable by-product.&quot;

5 inputs variables are gas and air flows.
2 outputs to predict are H2S and SO2 concentrations

See Appendix A.5 of Fortuna, L., Graziani, S., Rizzo, A., Xibilia, M.G. &quot;Soft Sensors for Monitoring and Control of Industrial Processes&quot; (Springer 2007) for more info.";;;
11;y2;23515;sulfur;numeric;0;1;"&quot;The sulfur recovery unit (SRU) removes environmental pollutants from acid gas
streams before they are released into the atmosphere. Furthermore, elemental sulfur
is recovered as a valuable by-product.&quot;

5 inputs variables are gas and air flows.
2 outputs to predict are H2S and SO2 concentrations

See Appendix A.5 of Fortuna, L., Graziani, S., Rizzo, A., Xibilia, M.G. &quot;Soft Sensors for Monitoring and Control of Industrial Processes&quot; (Springer 2007) for more info.";;;
12;productId;42183;dataset_sales;numeric;0;1;Dataset sales;;;
13;machineId;42183;dataset_sales;numeric;0;1;Dataset sales;;;
14;temp;42183;dataset_sales;numeric;0;1;Dataset sales;;;
15;weather_condition_id;42183;dataset_sales;numeric;0;1;Dataset sales;;;
16;isholiday;42183;dataset_sales;numeric;0;1;Dataset sales;;;
17;daysoff;42183;dataset_sales;numeric;0;1;Dataset sales;;;
18;year;42183;dataset_sales;numeric;0;1;Dataset sales;;;
19;month;42183;dataset_sales;numeric;0;1;Dataset sales;;;
20;day;42183;dataset_sales;numeric;0;1;Dataset sales;;;
21;week_day;42183;dataset_sales;numeric;0;1;Dataset sales;;;
22;avail0;42183;dataset_sales;numeric;0;1;Dataset sales;;;
23;avail1;42183;dataset_sales;numeric;0;1;Dataset sales;;;
24;avail2;42183;dataset_sales;numeric;0;1;Dataset sales;;;
25;sales;42183;dataset_sales;numeric;3;85;Dataset sales;;;
26;stdv;42183;dataset_sales;numeric;0;47;Dataset sales;;;
27;qts;42545;stock_fardamento02;numeric;1;833;"**Author**:   

**Source**: Unknown - Date unknown  

**Please cite**:   



valores de saida de fardamento com temperaturas, admiss&otilde;es e demiss&otilde;es";;;
28;Material;42545;stock_fardamento02;numeric;5367;65024;"**Author**:   

**Source**: Unknown - Date unknown  

**Please cite**:   



valores de saida de fardamento com temperaturas, admiss&otilde;es e demiss&otilde;es";;;
29;pp;42545;stock_fardamento02;numeric;0;20,1;"**Author**:   

**Source**: Unknown - Date unknown  

**Please cite**:   



valores de saida de fardamento com temperaturas, admiss&otilde;es e demiss&otilde;es";;;
30;TEMP;42545;stock_fardamento02;numeric;8,4;26;"**Author**:   

**Source**: Unknown - Date unknown  

**Please cite**:   



valores de saida de fardamento com temperaturas, admiss&otilde;es e demiss&otilde;es";;;
31;adm;42545;stock_fardamento02;numeric;28;230;"**Author**:   

**Source**: Unknown - Date unknown  

**Please cite**:   



valores de saida de fardamento com temperaturas, admiss&otilde;es e demiss&otilde;es";;;
32;Dem;42545;stock_fardamento02;numeric;36;239;"**Author**:   

**Source**: Unknown - Date unknown  

**Please cite**:   



valores de saida de fardamento com temperaturas, admiss&otilde;es e demiss&otilde;es";;;
33;1990;43809;GDP-per-capita-all-countries;numeric;424,823989;72906,52012;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
34;1991;43809;GDP-per-capita-all-countries;numeric;324,6360462;71753,72956;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
35;1992;43809;GDP-per-capita-all-countries;numeric;300,9789673;71567,82752;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
36;1993;43809;GDP-per-capita-all-countries;numeric;327,3411883;70082,38933;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
37;1994;43809;GDP-per-capita-all-countries;numeric;342,4329064;72471,68729;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
38;1995;43809;GDP-per-capita-all-countries;numeric;345,1363376;74994,38062;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
39;1996;43809;GDP-per-capita-all-countries;numeric;379,151098;76848,79224;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
40;1997;43809;GDP-per-capita-all-countries;numeric;417,8391769;80390,06411;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
41;1998;43809;GDP-per-capita-all-countries;numeric;452,6620933;77421,10866;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
42;1999;43809;GDP-per-capita-all-countries;numeric;469,1280301;76654,25591;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
43;2000;43809;GDP-per-capita-all-countries;numeric;455,8150878;86169,39432;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
44;2001;43809;GDP-per-capita-all-countries;numeric;443,5747121;88138,42244;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
45;2002;43809;GDP-per-capita-all-countries;numeric;450,4504171;92091,2766;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
46;2003;43809;GDP-per-capita-all-countries;numeric;469,7704098;91451,60854;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
47;2004;43809;GDP-per-capita-all-countries;numeric;498,9798703;101329,4787;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
48;2005;43809;GDP-per-capita-all-countries;numeric;528,9747838;97768,57115;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
49;2006;43809;GDP-per-capita-all-countries;numeric;555,799958;107541,4558;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
50;2007;43809;GDP-per-capita-all-countries;numeric;586,9952654;109362,5076;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
51;2008;43809;GDP-per-capita-all-countries;numeric;615,0719745;111255,9944;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
52;2009;43809;GDP-per-capita-all-countries;numeric;616,6586194;108953,6771;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
53;2010;43809;GDP-per-capita-all-countries;numeric;646,2953953;117518,7009;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
54;2011;43809;GDP-per-capita-all-countries;numeric;682,007332;124024,5682;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
55;2012;43809;GDP-per-capita-all-countries;numeric;719,914259;126618,3727;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
56;2013;43809;GDP-per-capita-all-countries;numeric;699,5928807;139962,1749;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
57;2014;43809;GDP-per-capita-all-countries;numeric;710,8121645;137832,9615;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
58;2015;43809;GDP-per-capita-all-countries;numeric;744,7345426;123822,0833;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
59;2016;43809;GDP-per-capita-all-countries;numeric;743,9035976;123573,6308;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
60;2017;43809;GDP-per-capita-all-countries;numeric;737,9785829;124609,3041;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
61;2018;43809;GDP-per-capita-all-countries;numeric;744,182072;126898,4259;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
62;2019;43809;GDP-per-capita-all-countries;numeric;;;"Gross Domestic Product (GDP) is the monetary value of all finished goods and services made within a country during a specific period. GDP provides an economic snapshot of a country, used to estimate the size of an economy and growth rate.
This dataset contains the GDP based on Purchasing Power Parity (PPP). 
GDP comparisons using PPP are arguably more useful than those using nominal GDP when assessing a nation's domestic market because PPP takes into account the relative cost of local goods, services and inflation rates of the country, rather than using international market exchange rates which may distort the real differences in per capita income
Acknowledgement
Thanks to World Databank";;;
63;Unnamed:_0;43825;Nutritional-values-for-common-foods-and-products;numeric;0;8788;"Context
I found this data occasionally and I just could not pass by. So I hope that this dasatet will help anyone who interested in food nutrition values.
Content
This dataset contains nutrition values for about 8.8k types of food. The features names is very self-explanatory, so I'll not make a description for them.";;;
64;calories;43825;Nutritional-values-for-common-foods-and-products;numeric;0;902;"Context
I found this data occasionally and I just could not pass by. So I hope that this dasatet will help anyone who interested in food nutrition values.
Content
This dataset contains nutrition values for about 8.8k types of food. The features names is very self-explanatory, so I'll not make a description for them.";;;
65;lucopene;43825;Nutritional-values-for-common-foods-and-products;numeric;0;0;"Context
I found this data occasionally and I just could not pass by. So I hope that this dasatet will help anyone who interested in food nutrition values.
Content
This dataset contains nutrition values for about 8.8k types of food. The features names is very self-explanatory, so I'll not make a description for them.";;;
66;Year;43831;Consumer-Price-Index;numeric;2013;2020;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
67;Cereals_and_products;43831;Consumer-Price-Index;numeric;107,5;152,7;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
68;Meat_and_fish;43831;Consumer-Price-Index;numeric;106,3;197;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
69;Egg;43831;Consumer-Price-Index;numeric;102,7;157;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
70;Milk_and_products;43831;Consumer-Price-Index;numeric;103,6;155,6;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
71;Oils_and_fats;43831;Consumer-Price-Index;numeric;101,1;138,2;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
72;Fruits;43831;Consumer-Price-Index;numeric;102,3;157,5;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
73;Vegetables;43831;Consumer-Price-Index;numeric;101,4;231,5;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
74;Pulses_and_products;43831;Consumer-Price-Index;numeric;103,5;191,6;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
75;Sugar_and_Confectionery;43831;Consumer-Price-Index;numeric;85,3;123,9;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
76;Spices;43831;Consumer-Price-Index;numeric;101,8;159,8;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
77;Non-alcoholic_beverages;43831;Consumer-Price-Index;numeric;104,8;142,1;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
78;Prepared_meals,_snacks,_sweets_etc.;43831;Consumer-Price-Index;numeric;106,7;161,8;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
79;Food_and_beverages;43831;Consumer-Price-Index;numeric;105,5;157;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
80;Pan,_tobacco_and_intoxicants;43831;Consumer-Price-Index;numeric;105,1;186,7;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
81;Clothing;43831;Consumer-Price-Index;numeric;105,9;154,7;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
82;Footwear;43831;Consumer-Price-Index;numeric;105;150;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
83;Clothing_and_footwear;43831;Consumer-Price-Index;numeric;105,8;154,1;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
84;Housing;43831;Consumer-Price-Index;numeric;100,3;155,6;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
85;Fuel_and_light;43831;Consumer-Price-Index;numeric;105,4;153,4;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
86;Household_goods_and_services;43831;Consumer-Price-Index;numeric;104,8;151,8;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
87;Health;43831;Consumer-Price-Index;numeric;104;158,2;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
88;Transport_and_communication;43831;Consumer-Price-Index;numeric;103,2;141,4;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
89;Recreation_and_amusement;43831;Consumer-Price-Index;numeric;102,9;153,2;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
90;Education;43831;Consumer-Price-Index;numeric;103,5;161,9;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
91;Personal_care_and_effects;43831;Consumer-Price-Index;numeric;102,1;152,2;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
92;Miscellaneous;43831;Consumer-Price-Index;numeric;103,7;151,7;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
93;General_index;43831;Consumer-Price-Index;numeric;104;152,7;Consumer Price Indices (CPI) measure changes over time in general level of prices of goods and services that households acquire for the purpose of consumption. CPI numbers are widely used as a macroeconomic indicator of inflation, as a tool by governments and central banks for inflation targeting and for monitoring price stability, and as deflators in the national accounts. CPI is also used for indexing dearness allowance to employees for increase in prices. CPI is therefore considered as one of the most important economic indicators. For construction of CPI numbers, two requisite components are weighting diagrams (consumption patterns) and price data collected at regular intervals. The data refers to group wise all India Consumer Price Index for Rural  Urban with base year 2010. The dataset is published by Central Statistical Office and released on 12th of every month.;;;
94;Unnamed:_0;43836;features-and-price-of-computer-components;numeric;0;919;"Context
The most common website that provided computer hardware components we chose the Newegg website it has hardware systems, Buy PC Parts, Laptops, Electronics  More. Now Shipping to Saudi Arabia! Track Order and more with fast shipping.
to determine which the best component with the best price here we can provide you this dataset. 
Content
Implement the Web scraping by using  the python language and using selenium on python, to extract the data from newegg that contain CPU, GPU, power, ram, monitor, storage 
the** data contains**:

the brand name
items_Decribtion
ratings
prices
Category (CPU, GPU,motherboard, ram, powersuplly, storage  )

Acknowledgements
Thank you for the MISK academy and general assembly for guiding us.
Inspiration
we recommend using EDA to clean data and also recommend to build model predictive price or build assumption analysis";;;
95;MEAN_TEMPERATURE_CALGARY;43843;Eighty-years-of-Canadian-climate-data;numeric;-37,5;26,2;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
96;TOTAL_PRECIPITATION_CALGARY;43843;Eighty-years-of-Canadian-climate-data;numeric;0;92,6;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
97;MEAN_TEMPERATURE_EDMONTON;43843;Eighty-years-of-Canadian-climate-data;numeric;-40,8;24,7;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
98;TOTAL_PRECIPITATION_EDMONTON;43843;Eighty-years-of-Canadian-climate-data;numeric;0;75,6;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
99;MEAN_TEMPERATURE_HALIFAX;43843;Eighty-years-of-Canadian-climate-data;numeric;-23,5;27;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
100;TOTAL_PRECIPITATION_HALIFAX;43843;Eighty-years-of-Canadian-climate-data;numeric;0;218,2;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
101;MEAN_TEMPERATURE_MONCTON;43843;Eighty-years-of-Canadian-climate-data;numeric;-27,4;27,2;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
102;TOTAL_PRECIPITATION_MONCTON;43843;Eighty-years-of-Canadian-climate-data;numeric;0;131,8;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
103;MEAN_TEMPERATURE_MONTREAL;43843;Eighty-years-of-Canadian-climate-data;numeric;-30,9;30,3;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
104;TOTAL_PRECIPITATION_MONTREAL;43843;Eighty-years-of-Canadian-climate-data;numeric;0;93,5;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
105;MEAN_TEMPERATURE_OTTAWA;43843;Eighty-years-of-Canadian-climate-data;numeric;-31,3;30;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
106;TOTAL_PRECIPITATION_OTTAWA;43843;Eighty-years-of-Canadian-climate-data;numeric;0;135,4;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
107;MEAN_TEMPERATURE_QUEBEC;43843;Eighty-years-of-Canadian-climate-data;numeric;-30,6;28,9;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
108;TOTAL_PRECIPITATION_QUEBEC;43843;Eighty-years-of-Canadian-climate-data;numeric;0;108,5;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
109;MEAN_TEMPERATURE_SASKATOON;43843;Eighty-years-of-Canadian-climate-data;numeric;-41,7;32,1;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
110;TOTAL_PRECIPITATION_SASKATOON;43843;Eighty-years-of-Canadian-climate-data;numeric;0;96,6;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
111;MEAN_TEMPERATURE_STJOHNS;43843;Eighty-years-of-Canadian-climate-data;numeric;-21,3;25,2;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
112;TOTAL_PRECIPITATION_STJOHNS;43843;Eighty-years-of-Canadian-climate-data;numeric;0;121,2;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
113;MEAN_TEMPERATURE_TORONTO;43843;Eighty-years-of-Canadian-climate-data;numeric;-24,7;31,8;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
114;TOTAL_PRECIPITATION_TORONTO;43843;Eighty-years-of-Canadian-climate-data;numeric;0;126;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
115;MEAN_TEMPERATURE_VANCOUVER;43843;Eighty-years-of-Canadian-climate-data;numeric;-14,5;28,4;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
116;TOTAL_PRECIPITATION_VANCOUVER;43843;Eighty-years-of-Canadian-climate-data;numeric;0;91,6;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
117;MEAN_TEMPERATURE_WHITEHORSE;43843;Eighty-years-of-Canadian-climate-data;numeric;-48,1;23,9;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
118;TOTAL_PRECIPITATION_WHITEHORSE;43843;Eighty-years-of-Canadian-climate-data;numeric;0;44,9;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
119;MEAN_TEMPERATURE_WINNIPEG;43843;Eighty-years-of-Canadian-climate-data;numeric;-38,6;30,9;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
120;TOTAL_PRECIPITATION_WINNIPEG;43843;Eighty-years-of-Canadian-climate-data;numeric;0;83,8;"This dataset has been compiled from public sources. The dataset consists of daily temperatures and precipitation from 13 Canadian centres. Precipitation is either rain or snow (likely snow in winter months). In 1940, there is daily data for seven out of the 13 centres, but by 1960 there is daily data from all 13 centres, with the occasional missing value.
Few of Canadas weather stations have been operating continuously, so we did need to patch together the data. Our source data is from https://climate-change.canada.ca/climate-data//daily-climate-data and here are the weather stations that we queried:
CALGARY INTL A
CALGARY INT'L A
EDMONTON INTL A
EDMONTON INT'L A
HALIFAX STANFIELD INT'L A
HALIFAX STANFIELD INT'L A
MONCTON A
MONCTON A
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL
MONTREAL/PIERRE ELLIOTT TRUDEAU INTL A
OTTAWA INTL A
OTTAWA MACDONALD-CARTIER INT'L A
QUEBEC/JEAN LESAGE INTL
QUEBEC/JEAN LESAGE INTL A
SASKATOON DIEFENBAKER INT'L A
SASKATOON INTL A
ST JOHN'S A
ST JOHNS WEST CLIMATE
TORONTO INTL A
TORONTO LESTER B. PEARSON INT'L A
VANCOUVER INTL A
VANCOUVER INT'L A
WHITEHORSE A
WHITEHORSE A
WINNIPEG RICHARDSON INT'L A
WINNIPEG THE FORKS
Suggested uses: The data is suitable for time series forecasting.  At Penny Analytics, we are using this dataset to demonstrate outlier detection (anomaly detection) in multiple time series and here is the relevant blogpost: https://pennyanalytics.com/2020/01/28/climate-change-canadian-weather-anomalies-are-now-warmer-and-wetter/";;;
121;total_cases;43844;Coronavirus-Worldwide-Dataset;numeric;0;20075600;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
122;new_cases;43844;Coronavirus-Worldwide-Dataset;numeric;-2461;298083;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
123;total_deaths;43844;Coronavirus-Worldwide-Dataset;numeric;0;736372;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
124;new_deaths;43844;Coronavirus-Worldwide-Dataset;numeric;-1918;10504;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
125;total_cases_per_million;43844;Coronavirus-Worldwide-Dataset;numeric;0;39312,614;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
126;new_cases_per_million;43844;Coronavirus-Worldwide-Dataset;numeric;-265,189;4944,376;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
127;total_deaths_per_million;43844;Coronavirus-Worldwide-Dataset;numeric;0;1237,551;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
128;new_deaths_per_million;43844;Coronavirus-Worldwide-Dataset;numeric;-41,023;200,04;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
129;new_tests;43844;Coronavirus-Worldwide-Dataset;numeric;-3743;926876;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
130;total_tests;43844;Coronavirus-Worldwide-Dataset;numeric;1;61792571;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
131;total_tests_per_thousand;43844;Coronavirus-Worldwide-Dataset;numeric;0;707,957;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
132;new_tests_per_thousand;43844;Coronavirus-Worldwide-Dataset;numeric;-0,398;22,359;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
133;new_tests_smoothed;43844;Coronavirus-Worldwide-Dataset;numeric;0;822470;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
134;new_tests_smoothed_per_thousand;43844;Coronavirus-Worldwide-Dataset;numeric;0;15,82;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
135;tests_per_case;43844;Coronavirus-Worldwide-Dataset;numeric;1,398;47299;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
136;positive_rate;43844;Coronavirus-Worldwide-Dataset;numeric;0;0,715;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
137;stringency_index;43844;Coronavirus-Worldwide-Dataset;numeric;0;100;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
138;population;43844;Coronavirus-Worldwide-Dataset;numeric;809;7794798729;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
139;population_density;43844;Coronavirus-Worldwide-Dataset;numeric;0,137;19347,5;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
140;median_age;43844;Coronavirus-Worldwide-Dataset;numeric;15,1;48,2;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
141;aged_65_older;43844;Coronavirus-Worldwide-Dataset;numeric;1,144;27,049;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
142;aged_70_older;43844;Coronavirus-Worldwide-Dataset;numeric;0,526;18,493;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
143;gdp_per_capita;43844;Coronavirus-Worldwide-Dataset;numeric;661,24;116935,6;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
144;extreme_poverty;43844;Coronavirus-Worldwide-Dataset;numeric;0,1;77,6;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
145;cardiovasc_death_rate;43844;Coronavirus-Worldwide-Dataset;numeric;79,37;724,417;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
146;diabetes_prevalence;43844;Coronavirus-Worldwide-Dataset;numeric;0,99;23,36;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
147;female_smokers;43844;Coronavirus-Worldwide-Dataset;numeric;0,1;44;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
148;male_smokers;43844;Coronavirus-Worldwide-Dataset;numeric;7,7;78,1;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
149;handwashing_facilities;43844;Coronavirus-Worldwide-Dataset;numeric;1,188;98,999;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
150;hospital_beds_per_thousand;43844;Coronavirus-Worldwide-Dataset;numeric;0,1;13,8;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
151;life_expectancy;43844;Coronavirus-Worldwide-Dataset;numeric;53,28;86,75;"Context
From World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.
So daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.
The European CDC publishes daily statistics on the COVID-19 pandemic. Not just for Europe, but for the entire world. We rely on the ECDC as they collect and harmonize data from around the world which allows us to compare what is happening in different countries.
Content
This dataset has daily level information on the number of affected cases, deaths and recovery etc. from coronavirus. It also contains various other parameters like average life expectancy, population density, smocking population etc. which users can find useful in further prediction that they need to make.
The data is available from 31 Dec,2019.
Inspiration
Give people weekly data so that they can use it to make accurate predictions.";;;
152;Unnamed:_0;43845;Music-Dataset--1950-to-2019;numeric;0;82451;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
153;release_date;43845;Music-Dataset--1950-to-2019;numeric;1950;2019;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
154;len;43845;Music-Dataset--1950-to-2019;numeric;1;199;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
155;dating;43845;Music-Dataset--1950-to-2019;numeric;0,000290782;0,647705684;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
156;violence;43845;Music-Dataset--1950-to-2019;numeric;0,000284495;0,981781376;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
157;world/life;43845;Music-Dataset--1950-to-2019;numeric;0,000290782;0,962105263;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
158;night/time;43845;Music-Dataset--1950-to-2019;numeric;0,000289185;0,97368421;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
159;shake_the_audience;43845;Music-Dataset--1950-to-2019;numeric;0,000284495;0,497463245;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
160;family/gospel;43845;Music-Dataset--1950-to-2019;numeric;0,000289185;0,54530302;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
161;romantic;43845;Music-Dataset--1950-to-2019;numeric;0,000284495;0,940789474;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
162;communication;43845;Music-Dataset--1950-to-2019;numeric;0,000290782;0,645829372;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
163;obscene;43845;Music-Dataset--1950-to-2019;numeric;0,000289185;0,992297817;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
164;music;43845;Music-Dataset--1950-to-2019;numeric;0,000289185;0,956937798;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
165;movement/places;43845;Music-Dataset--1950-to-2019;numeric;0,000284495;0,638020876;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
166;light/visual_perceptions;43845;Music-Dataset--1950-to-2019;numeric;0,000284495;0,667781877;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
167;family/spiritual;43845;Music-Dataset--1950-to-2019;numeric;0,000284495;0,618072522;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
168;like/girls;43845;Music-Dataset--1950-to-2019;numeric;0,000284495;0,594458741;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
169;sadness;43845;Music-Dataset--1950-to-2019;numeric;0,000284495;0,981424148;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
170;feelings;43845;Music-Dataset--1950-to-2019;numeric;0,000289185;0,958810069;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
171;danceability;43845;Music-Dataset--1950-to-2019;numeric;0,005415358;0,99350157;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
172;loudness;43845;Music-Dataset--1950-to-2019;numeric;0;1;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
173;acousticness;43845;Music-Dataset--1950-to-2019;numeric;2,81E-07;1;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
174;instrumentalness;43845;Music-Dataset--1950-to-2019;numeric;0;0,996963563;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
175;valence;43845;Music-Dataset--1950-to-2019;numeric;0;1;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
176;energy;43845;Music-Dataset--1950-to-2019;numeric;0;1;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
177;age;43845;Music-Dataset--1950-to-2019;numeric;0,014285714;1;"Context
This dataset provides a list of lyrics from 1950 to 2019 describing music metadata as sadness, danceability, loudness, acousticness, etc. Authors also provide some information as lyrics which can be used to natural language processing. 
Acknowledgements
Moura, Luan; Fontelles, Emanuel; Sampaio, Vinicius; Frana, Mardnio (2020), Music Dataset: Lyrics and Metadata from 1950 to 2019, Mendeley Data, V3, doi: 10.17632/3t9vbwxgr5.3";;;
178;age;43904;law-school-admission-bianry;numeric;3;69;"Law School Admissions (Binarized) 
 Survey among students attending law school in the U.S. in 1991. 
 The dataset was obtained from the R-package fairml. 
 The response variable has been changed to a binary version: Whether ugpa is greater than 3.
 The race1 variables has been binarized to labels white and non-white.";;;
179;decile1;43904;law-school-admission-bianry;numeric;1;10;"Law School Admissions (Binarized) 
 Survey among students attending law school in the U.S. in 1991. 
 The dataset was obtained from the R-package fairml. 
 The response variable has been changed to a binary version: Whether ugpa is greater than 3.
 The race1 variables has been binarized to labels white and non-white.";;;
180;decile3;43904;law-school-admission-bianry;numeric;1;10;"Law School Admissions (Binarized) 
 Survey among students attending law school in the U.S. in 1991. 
 The dataset was obtained from the R-package fairml. 
 The response variable has been changed to a binary version: Whether ugpa is greater than 3.
 The race1 variables has been binarized to labels white and non-white.";;;
181;fam_inc;43904;law-school-admission-bianry;numeric;1;5;"Law School Admissions (Binarized) 
 Survey among students attending law school in the U.S. in 1991. 
 The dataset was obtained from the R-package fairml. 
 The response variable has been changed to a binary version: Whether ugpa is greater than 3.
 The race1 variables has been binarized to labels white and non-white.";;;
182;lsat;43904;law-school-admission-bianry;numeric;11;48;"Law School Admissions (Binarized) 
 Survey among students attending law school in the U.S. in 1991. 
 The dataset was obtained from the R-package fairml. 
 The response variable has been changed to a binary version: Whether ugpa is greater than 3.
 The race1 variables has been binarized to labels white and non-white.";;;
183;CreditScore;45062;shrutime;numeric;350;850;This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.Source: https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling;;;
184;Age;45062;shrutime;numeric;18;92;This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.Source: https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling;;;
185;Balance;45062;shrutime;numeric;0;250898,09;This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.Source: https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling;;;
186;EstimatedSalary;45062;shrutime;numeric;11,58;199992,48;This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.Source: https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling;;;
187;class;45062;shrutime;numeric;0;1;This data set contains details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer.Source: https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling;;;
188;year;45106;MTPL_SHAP_Tutorial;numeric;2018;2019;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
189;town;45106;MTPL_SHAP_Tutorial;numeric;0;1;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
190;driver_age;45106;MTPL_SHAP_Tutorial;numeric;18;88;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
191;car_weight;45106;MTPL_SHAP_Tutorial;numeric;950;3120;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
192;car_power;45106;MTPL_SHAP_Tutorial;numeric;50;341;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
193;car_age;45106;MTPL_SHAP_Tutorial;numeric;0;23;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
194;claim_nb;45106;MTPL_SHAP_Tutorial;numeric;0;4;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
195;year;45106;MTPL_SHAP_Tutorial;numeric;2018;2019;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
196;town;45106;MTPL_SHAP_Tutorial;numeric;0;1;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
197;driver_age;45106;MTPL_SHAP_Tutorial;numeric;18;88;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
198;car_weight;45106;MTPL_SHAP_Tutorial;numeric;950;3120;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
199;car_power;45106;MTPL_SHAP_Tutorial;numeric;50;341;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
200;car_age;45106;MTPL_SHAP_Tutorial;numeric;0;23;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
201;claim_nb;45106;MTPL_SHAP_Tutorial;numeric;0;4;"This motor third-part liability (MTPL) pricing dataset describes 1 Mio insurance policies and their corresponding claim counts, see
      Mayer, M., Meier, D. and Wuthrich, M.V. (2023) SHAP for Actuaries: Explain any Model. http://dx.doi.org/10.2139/ssrn.4389797";;;
202;normalized-losses;45080;autos;numeric;37,625922;273,722584;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
203;wheel-base;45080;autos;numeric;83,069816;127,194382;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
204;length;45080;autos;numeric;134,019251;216,548564;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
205;width;45080;autos;numeric;60,270307;75,596629;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
206;height;45080;autos;numeric;46,979469;62,50061;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
207;curb-weight;45080;autos;numeric;1509,552607;4716,212861;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
208;engine-size;45080;autos;numeric;9,891578;381,012483;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
209;bore;45080;autos;numeric;2,554261;4,072103;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
210;stroke;45080;autos;numeric;1,713911;4,458423;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
211;compression-ratio;45080;autos;numeric;-11,806341;42,161764;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
212;horsepower;45080;autos;numeric;39,520425;303,922617;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
213;peak-rpm;45080;autos;numeric;3406,968387;6931,555398;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
214;city-mpg;45080;autos;numeric;11,110762;54,824617;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
215;highway-mpg;45080;autos;numeric;11,470231;61,696829;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
216;price;45080;autos;numeric;-12038,16054;63918,81917;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
217;class;45080;autos;numeric;-3;3;"This data set consists of three types of entities: (a) the specification of an auto in terms of various characteristics, (b) its assigned insurance risk rating, (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc), and represents the average loss per car per year.Note: Several of the attributes in the database could be used as a ""class"" attribute.Source: https://www.kaggle.com/datasets/toramky/automobile-dataset";;;
218;Vehicle_Reference_df_res;45079;road-safety;numeric;1;37;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
219;Vehicle_Type;45079;road-safety;numeric;2;98;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
220;Vehicle_Manoeuvre;45079;road-safety;numeric;1;18;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
221;Vehicle_Location-Restricted_Lane;45079;road-safety;numeric;0;9;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
222;Hit_Object_in_Carriageway;45079;road-safety;numeric;0;12;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
223;Hit_Object_off_Carriageway;45079;road-safety;numeric;0;11;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
224;Age_of_Driver;45079;road-safety;numeric;12;97;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
225;Age_Band_of_Driver;45079;road-safety;numeric;3;11;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
226;Engine_Capacity_(CC);45079;road-safety;numeric;48;18000;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
227;Propulsion_Code;45079;road-safety;numeric;1;12;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
228;Age_of_Vehicle;45079;road-safety;numeric;1;91;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
229;Location_Easting_OSGR;45079;road-safety;numeric;90609;655282;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
230;Location_Northing_OSGR;45079;road-safety;numeric;10628;1142228;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
231;Longitude;45079;road-safety;numeric;-6,311427;1,758443;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
232;Latitude;45079;road-safety;numeric;49,915618;60,161843;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
233;Police_Force;45079;road-safety;numeric;1;98;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
234;Number_of_Vehicles;45079;road-safety;numeric;1;37;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
235;Number_of_Casualties;45079;road-safety;numeric;1;38;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
236;Local_Authority_(District);45079;road-safety;numeric;1;940;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
237;1st_Road_Number;45079;road-safety;numeric;0;9914;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
238;2nd_Road_Number;45079;road-safety;numeric;0;9999;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
239;Vehicle_Reference_df;45079;road-safety;numeric;1;27;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
240;Casualty_Reference;45079;road-safety;numeric;1;38;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
241;Age_of_Casualty;45079;road-safety;numeric;0;101;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
242;Age_Band_of_Casualty;45079;road-safety;numeric;1;11;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
243;Pedestrian_Location;45079;road-safety;numeric;0;10;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
244;Pedestrian_Movement;45079;road-safety;numeric;0;9;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
245;Casualty_Type;45079;road-safety;numeric;0;98;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
246;Casualty_IMD_Decile;45079;road-safety;numeric;1;10;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
247;class;45079;road-safety;numeric;1;2;Data reported to the police about the circumstances of personal injury road accidents in Great Britain from 1979, and the maker and model information of vehicles involved in the respective accident.This version includes data up to 2015.;;;
248;0;45077;qsar;numeric;2;6,496;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
249;1;45077;qsar;numeric;0,8039;9,1775;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
250;7;45077;qsar;numeric;0;60,7;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
251;11;45077;qsar;numeric;-5,256;4,722;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
252;12;45077;qsar;numeric;1,544;5,701;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
253;13;45077;qsar;numeric;0;4,491;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
254;14;45077;qsar;numeric;4,174;12,609;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
255;16;45077;qsar;numeric;0,957;1,311;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
256;17;45077;qsar;numeric;1,022;1,377;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
257;21;45077;qsar;numeric;0,863;1,641;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
258;26;45077;qsar;numeric;1;2,859;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
259;27;45077;qsar;numeric;-1,099;1,073;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
260;29;45077;qsar;numeric;0;71,167;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
261;30;45077;qsar;numeric;0,444;17,537;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
262;35;45077;qsar;numeric;2,267;10,695;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
263;36;45077;qsar;numeric;1,467;5,825;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
264;38;45077;qsar;numeric;4,917;14,7;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
265;2;45077;qsar;numeric;0;12;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
266;4;45077;qsar;numeric;0;36;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
267;5;45077;qsar;numeric;0;13;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
268;6;45077;qsar;numeric;0;18;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
269;8;45077;qsar;numeric;0;24;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
270;9;45077;qsar;numeric;0;12;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
271;10;45077;qsar;numeric;0;44;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
272;15;45077;qsar;numeric;0;40;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
273;31;45077;qsar;numeric;0;8;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
274;32;45077;qsar;numeric;0;12;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
275;33;45077;qsar;numeric;0;18;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
276;37;45077;qsar;numeric;0;8;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
277;40;45077;qsar;numeric;0;27;The QSAR biodegradation dataset was built in the Milano Chemometrics and QSAR Research Group. The research leading to these results has received funding from the European Communitys Seventh Framework Programme [FP7/2007-2013] under Grant Agreement n. 238701 of Marie Curie ITN Environmental Chemoinformatics (ECO) project.The data have been used to develop QSAR (Quantitative Structure Activity Relationships) models for the study of the relationships between chemical structure and biodegradation of molecules. Biodegradation experimental values of 1055 chemicals were collected from the webpage of the National Institute of Technology and Evaluation of Japan (NITE). Classification models were developed in order to discriminate ready (356) and not ready (699) biodegradable molecules by means of three different modelling methods: k Nearest Neighbours, Partial Least Squares Discriminant Analysis and Support Vector Machines. Details on attributes (molecular descriptors) selected in each model can be found in the quoted reference: Mansouri, K., Ringsted, T., Ballabio, D., Todeschini, R., Consonni, V. (2013). Quantitative Structure - Activity Relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling, 53, 867-878.Source: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation;;;
278;Customer_care_calls;45074;Shipping;numeric;2;7;An international e-commerce company based wants to discover key insights from their customer database. They want to use some of the most advanced machine learning techniques to study their customers. The company sells electronic products;;;
279;Customer_rating;45074;Shipping;numeric;1;5;An international e-commerce company based wants to discover key insights from their customer database. They want to use some of the most advanced machine learning techniques to study their customers. The company sells electronic products;;;
280;Prior_purchases;45074;Shipping;numeric;2;10;An international e-commerce company based wants to discover key insights from their customer database. They want to use some of the most advanced machine learning techniques to study their customers. The company sells electronic products;;;
281;Discount_offered;45074;Shipping;numeric;1;65;An international e-commerce company based wants to discover key insights from their customer database. They want to use some of the most advanced machine learning techniques to study their customers. The company sells electronic products;;;
282;Weight_in_gms;45074;Shipping;numeric;1001;7846;An international e-commerce company based wants to discover key insights from their customer database. They want to use some of the most advanced machine learning techniques to study their customers. The company sells electronic products;;;
283;class;45074;Shipping;numeric;0;1;An international e-commerce company based wants to discover key insights from their customer database. They want to use some of the most advanced machine learning techniques to study their customers. The company sells electronic products;;;
284;lineNo;45073;eye_movements;numeric;1;10927;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
285;assgNo;45073;eye_movements;numeric;1;336;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
286;prevFixDur;45073;eye_movements;numeric;0;1036;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
287;firstfixDur;45073;eye_movements;numeric;20;777;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
288;firstPassFixDur;45073;eye_movements;numeric;20;1392;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
289;nextFixDur;45073;eye_movements;numeric;0;1133;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
290;firstSaccLen;45073;eye_movements;numeric;0;1686,4404;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
291;lastSaccLen;45073;eye_movements;numeric;0;1924,1347;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
292;prevFixPos;45073;eye_movements;numeric;0;1070,3616;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
293;landingPos;45073;eye_movements;numeric;1,118;1348,7161;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
294;leavingPos;45073;eye_movements;numeric;0,5;1343,5316;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
295;totalFixDur;45073;eye_movements;numeric;20;1392;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
296;meanFixDur;45073;eye_movements;numeric;20;757;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
297;regressLen;45073;eye_movements;numeric;0;24987;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
298;regressDur;45073;eye_movements;numeric;0;11140;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
299;pupilDiamMax;45073;eye_movements;numeric;-4,3255;4,4917;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
300;pupilDiamLag;45073;eye_movements;numeric;-1,206;4,4917;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
301;timePrtctg;45073;eye_movements;numeric;0,0005;0,318;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
302;titleNo;45073;eye_movements;numeric;1;10;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
303;wordNo;45073;eye_movements;numeric;1;10;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
304;class;45073;eye_movements;numeric;0;1;Jarkko Salojarvi, Kai Puolamaki, Jaana Simola, Lauri Kovanen, Ilpo Kojo, Samuel Kaski. Inferring Relevance from Eye Movements: Feature Extraction. Helsinki University of Technology, Publications in Computer and Information Science, Report A82. 3 March 2005. Data set at http://www.cis.hut.fi/eyechallenge2005/Competition 1 (preprocessed data) A straight-forward classification task. We provide pre-computed feature vectors for each word in the eye movement trajectory, with class labels.The dataset consist of several assignments. Each assignment consists of a question followed by ten sentences (titles of news articles). One of the sentences is the correct answer to the question (C) and five of the sentences are irrelevant to the question (I). Four of the sentences are relevant to the question (R), but they do not answer it.;;;
305;wage_eur;45012;fifa;numeric;500;350000;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
306;age;45012;fifa;numeric;16;54;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
307;height_cm;45012;fifa;numeric;155;206;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
308;weight_kg;45012;fifa;numeric;49;110;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
309;overall;45012;fifa;numeric;47;93;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
310;potential;45012;fifa;numeric;49;95;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
311;attacking_crossing;45012;fifa;numeric;6;94;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
312;attacking_finishing;45012;fifa;numeric;2;95;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
313;attacking_heading_accuracy;45012;fifa;numeric;5;93;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
314;attacking_short_passing;45012;fifa;numeric;7;94;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
315;attacking_volleys;45012;fifa;numeric;3;90;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
316;skill_dribbling;45012;fifa;numeric;4;96;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
317;skill_curve;45012;fifa;numeric;6;94;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
318;skill_fk_accuracy;45012;fifa;numeric;4;94;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
319;skill_long_passing;45012;fifa;numeric;9;93;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
320;skill_ball_control;45012;fifa;numeric;8;96;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
321;movement_acceleration;45012;fifa;numeric;14;97;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
322;movement_sprint_speed;45012;fifa;numeric;15;97;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
323;movement_agility;45012;fifa;numeric;18;96;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
324;movement_reactions;45012;fifa;numeric;25;94;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
325;movement_balance;45012;fifa;numeric;15;96;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
326;defending_standing_tackle;45012;fifa;numeric;5;93;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
327;defending_sliding_tackle;45012;fifa;numeric;5;92;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
328;goalkeeping_diving;45012;fifa;numeric;2;91;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
329;goalkeeping_handling;45012;fifa;numeric;2;92;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
330;goalkeeping_kicking;45012;fifa;numeric;2;93;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
331;goalkeeping_positioning;45012;fifa;numeric;2;92;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
332;goalkeeping_reflexes;45012;fifa;numeric;2;90;"**Data Description**

The datasets provided include the players data for the Career Mode from FIFA 22.
It only includes male football players with known wage.

The goal is to predict the wage of the player based on his attributes.

**Attribute Description**

The features describe self-explanatory properties and skills of the player.";;;
333;passenger_count;44986;nyc_taxi_green;numeric;0;9;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";"Minimum: 1
Maximum: 10";;
334;tip_amount;44986;nyc_taxi_green;numeric;-10,56;250,7;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";"Minimum: 0.01
Maximum: 100.00";;
335;tolls_amount;44986;nyc_taxi_green;numeric;0;98;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";"Minimum value: 0

Maximum value: 100";;
336;total_amount;44986;nyc_taxi_green;numeric;-63,36;712,38;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";"Minimum: 0
Maximum: 1000000";;
337;lpep_pickup_datetime_day;44986;nyc_taxi_green;numeric;1;31;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";Min: 1, Max: 31;;
338;lpep_pickup_datetime_hour;44986;nyc_taxi_green;numeric;0;23;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";"Minimum: 0
Maximum: 23";;
339;lpep_pickup_datetime_minute;44986;nyc_taxi_green;numeric;0;59;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";"Minimum: 0 
Maximum: 59";;
340;lpep_dropoff_datetime_day;44986;nyc_taxi_green;numeric;1;31;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";"Minimum: 1
Maximum: 31";;
341;lpep_dropoff_datetime_hour;44986;nyc_taxi_green;numeric;0;23;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";"Minimum: 0 
Maximum: 23";;
342;lpep_dropoff_datetime_minute;44986;nyc_taxi_green;numeric;0;59;"**Data Description**

The dataset includes New York City Taxi and Limousine Commission (TLC) trips of the green line in December 2016. All trips are paid with a credit card leaving some tip.

The variable 'tip_amount' was chosen as target variable.

**Attribute Description**

1. *VendorID* - A code indicating the LPEP provider that provided the record. 1: Creative Mobile Technologies, LLC; 2: VeriFone Inc.
2. *store_and_fwd_flag*
3. *RatecodeID*
4. *PULocationID*
5. *DOLocationID* - TLC Taxi Zone in which the taximeter was disengaged.
6. *passenger_count* - the number of passengers in the vehicle. This is a driver-entered value
7. *extra* - miscellaneous extras and surcharges. Currently, this only includes the $0.50 and $1 rush hour and overnight charges
8. *mta_tax* - $0.50 MTA tax that is automatically triggered based on the metered rate in use.
9. *tip_amount* - target feature
10. *tolls_amount*
11. *improvement_surcharge* - $0.30 improvement surcharge assessed on hailed trips at the flag drop
12. *total_amount*
13. *trip_type* - 1: Street-hail, 2: Dispatch
14. *lpep_pickup_datetime_day*
15. *lpep_pickup_datetime_hour*
16. *lpep_pickup_datetime_minute*
17. *lpep_dropoff_datetime_day*
18. *lpep_dropoff_datetime_hour*
19. *lpep_dropoff_datetime_minute*";"Minimum: 0
Maximum: 59";;
343;ln_votes_pop;44985;space_ga;numeric;-3,057045019;0,100083459;"**Data Description**

The dataset contains 3,107 observations on U.S. county votes cast in the 1980 presidential election.

Given population, education levels in the population, number of owned houses, incomes and coordinates, the goal is to predict the number of votes per population (logarithmic).

**Attribute Description**

1. *ln_votes_pop* - logarithm of total number of votes - logarithm of populaiton (ln(votes/pop)), target feature
2. *pop* - the population in each county of 18 years of age or older
3. *education* - the population in each county with a 12th grade or higher education
4. *houses* - the number of owner-occupied housing units
5. *income* - the aggregate income
6. *xcoord* - X spatial coordinate of the county
7. *ycoord* - Y spatial coordinate of the county";"Minimum: Unknown
Maximum: Unknown";;
344;pop;44985;space_ga;numeric;4,369447852;15,51043143;"**Data Description**

The dataset contains 3,107 observations on U.S. county votes cast in the 1980 presidential election.

Given population, education levels in the population, number of owned houses, incomes and coordinates, the goal is to predict the number of votes per population (logarithmic).

**Attribute Description**

1. *ln_votes_pop* - logarithm of total number of votes - logarithm of populaiton (ln(votes/pop)), target feature
2. *pop* - the population in each county of 18 years of age or older
3. *education* - the population in each county with a 12th grade or higher education
4. *houses* - the number of owner-occupied housing units
5. *income* - the aggregate income
6. *xcoord* - X spatial coordinate of the county
7. *ycoord* - Y spatial coordinate of the county";"Minimum: 0
Maximum: 1,000,000";;
345;education;44985;space_ga;numeric;3,63758616;14,94512462;"**Data Description**

The dataset contains 3,107 observations on U.S. county votes cast in the 1980 presidential election.

Given population, education levels in the population, number of owned houses, incomes and coordinates, the goal is to predict the number of votes per population (logarithmic).

**Attribute Description**

1. *ln_votes_pop* - logarithm of total number of votes - logarithm of populaiton (ln(votes/pop)), target feature
2. *pop* - the population in each county of 18 years of age or older
3. *education* - the population in each county with a 12th grade or higher education
4. *houses* - the number of owner-occupied housing units
5. *income* - the aggregate income
6. *xcoord* - X spatial coordinate of the county
7. *ycoord* - Y spatial coordinate of the county";"Minimum: High school or equivalent
Maximum: Doctorate";;
346;houses;44985;space_ga;numeric;3,135494216;14,09571247;"**Data Description**

The dataset contains 3,107 observations on U.S. county votes cast in the 1980 presidential election.

Given population, education levels in the population, number of owned houses, incomes and coordinates, the goal is to predict the number of votes per population (logarithmic).

**Attribute Description**

1. *ln_votes_pop* - logarithm of total number of votes - logarithm of populaiton (ln(votes/pop)), target feature
2. *pop* - the population in each county of 18 years of age or older
3. *education* - the population in each county with a 12th grade or higher education
4. *houses* - the number of owner-occupied housing units
5. *income* - the aggregate income
6. *xcoord* - X spatial coordinate of the county
7. *ycoord* - Y spatial coordinate of the county";"The minimum and maximum values for the feature ""house"" in the dataset cannot be determined without access to the actual data.";;
347;income;44985;space_ga;numeric;7,586803535;17,94399105;"**Data Description**

The dataset contains 3,107 observations on U.S. county votes cast in the 1980 presidential election.

Given population, education levels in the population, number of owned houses, incomes and coordinates, the goal is to predict the number of votes per population (logarithmic).

**Attribute Description**

1. *ln_votes_pop* - logarithm of total number of votes - logarithm of populaiton (ln(votes/pop)), target feature
2. *pop* - the population in each county of 18 years of age or older
3. *education* - the population in each county with a 12th grade or higher education
4. *houses* - the number of owner-occupied housing units
5. *income* - the aggregate income
6. *xcoord* - X spatial coordinate of the county
7. *ycoord* - Y spatial coordinate of the county";"Minimum Income: $20,000
Maximum Income: $200,000";;
348;xcoord;44985;space_ga;numeric;-124229902;-67609990;"**Data Description**

The dataset contains 3,107 observations on U.S. county votes cast in the 1980 presidential election.

Given population, education levels in the population, number of owned houses, incomes and coordinates, the goal is to predict the number of votes per population (logarithmic).

**Attribute Description**

1. *ln_votes_pop* - logarithm of total number of votes - logarithm of populaiton (ln(votes/pop)), target feature
2. *pop* - the population in each county of 18 years of age or older
3. *education* - the population in each county with a 12th grade or higher education
4. *houses* - the number of owner-occupied housing units
5. *income* - the aggregate income
6. *xcoord* - X spatial coordinate of the county
7. *ycoord* - Y spatial coordinate of the county";"Minimum: -100
Maximum: 100";;
349;ycoord;44985;space_ga;numeric;25117067;48833747;"**Data Description**

The dataset contains 3,107 observations on U.S. county votes cast in the 1980 presidential election.

Given population, education levels in the population, number of owned houses, incomes and coordinates, the goal is to predict the number of votes per population (logarithmic).

**Attribute Description**

1. *ln_votes_pop* - logarithm of total number of votes - logarithm of populaiton (ln(votes/pop)), target feature
2. *pop* - the population in each county of 18 years of age or older
3. *education* - the population in each county with a 12th grade or higher education
4. *houses* - the number of owner-occupied housing units
5. *income* - the aggregate income
6. *xcoord* - X spatial coordinate of the county
7. *ycoord* - Y spatial coordinate of the county";"Minimum: -∞
Maximum: +∞";;
350;theta1;44981;pumadyn32nh;numeric;-2,3559788;2,3560933;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";"Minimum: -10
Maximum: 10";;
351;theta2;44981;pumadyn32nh;numeric;-2,3560923;2,3559392;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";"Minimum: -180 degrees
Maximum: 180 degrees";;
352;theta3;44981;pumadyn32nh;numeric;-2,3561923;2,3560761;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";"Minimum: -10
Maximum: 10";;
353;theta4;44981;pumadyn32nh;numeric;-2,3561554;2,3554484;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";"Minimum: -10
Maximum: 10";;
354;theta5;44981;pumadyn32nh;numeric;-2,3558931;2,3557348;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";"Minimum value of theta5: -∞
Maximum value of theta5: +∞";;
355;theta6;44981;pumadyn32nh;numeric;-2,356149;2,3555894;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";Sorry, but I can't provide the minimum and maximum values for the feature theta6 without any information about the dataset.;;
356;thetad1;44981;pumadyn32nh;numeric;-2,3558835;2,3561468;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";"Minimum: -π (pi)
Maximum: π (pi)";;
357;thetad2;44981;pumadyn32nh;numeric;-2,3553779;2,3558753;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";"Minimum: -180
Maximum: 180";;
358;thetad3;44981;pumadyn32nh;numeric;-2,3555789;2,3558805;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
359;thetad4;44981;pumadyn32nh;numeric;-2,3557642;2,3561293;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
360;thetad5;44981;pumadyn32nh;numeric;-2,3559181;2,3559142;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
361;thetad6;44981;pumadyn32nh;numeric;-2,3556409;2,3558045;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
362;tau1;44981;pumadyn32nh;numeric;-74,986429;74,982066;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
363;tau2;44981;pumadyn32nh;numeric;-74,985392;74,98269;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
364;tau3;44981;pumadyn32nh;numeric;-74,99187;74,963215;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
365;tau4;44981;pumadyn32nh;numeric;-74,996111;74,965567;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
366;tau5;44981;pumadyn32nh;numeric;-74,994011;74,964614;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
367;dm1;44981;pumadyn32nh;numeric;0,2503663;2,4998416;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
368;dm2;44981;pumadyn32nh;numeric;0,25009217;2,499883;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
369;dm3;44981;pumadyn32nh;numeric;0,25050915;2,4999468;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
370;dm4;44981;pumadyn32nh;numeric;0,2500787;2,4998424;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
371;dm5;44981;pumadyn32nh;numeric;0,25074914;2,4995216;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
372;da1;44981;pumadyn32nh;numeric;0,25021364;2,4997496;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
373;da2;44981;pumadyn32nh;numeric;0,25014345;2,4999086;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
374;da3;44981;pumadyn32nh;numeric;0,25043444;2,4998912;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
375;da4;44981;pumadyn32nh;numeric;0,25008951;2,4998174;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
376;da5;44981;pumadyn32nh;numeric;0,25052101;2,4997306;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
377;db1;44981;pumadyn32nh;numeric;0,25015474;2,4999003;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
378;db2;44981;pumadyn32nh;numeric;0,25065343;2,499937;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
379;db3;44981;pumadyn32nh;numeric;0,25008243;2,4995813;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
380;db4;44981;pumadyn32nh;numeric;0,2504841;2,4995562;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
381;db5;44981;pumadyn32nh;numeric;0,25003647;2,4996671;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
382;thetadd6;44981;pumadyn32nh;numeric;-0,11351719;0,13014103;"**Data Description**

A realistic simulation of the dynamics of a Puma 560 robot arm. The task in this dataset is to predict the angular acceleration of one of the robot arm's links. The task is nonlinear and has high noise.

The similator got as input the angular positions, velocities and torques and other dynamic parameters of the Puma arm.

The goal was to predict the acceleration of link 6 *thetad6*.

**Attribute Description**

1. *theta[1-6]* - angular positions of links 1 to 6
2. *thetad[1-6]* - angular velocities of links 1 to 6
3. *tau[1-5]* - torques at joints 1 to 5
4. *dm[1-5]* - change in mass of links 1 to 5
5. *da[1-5]* - change in length of links 1 to 5
6. *db[1-5]* - change in viscous friction of links 1 - 5
7. *thetadd6* - acceleration of link 6, target feature";;;
383;RMSD;44963;physiochemical_protein;numeric;0;20,999;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
384;F1;44963;physiochemical_protein;numeric;2392,05;40034,9;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
385;F2;44963;physiochemical_protein;numeric;403,5;15312;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
386;F3;44963;physiochemical_protein;numeric;0,0925;0,57769;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
387;F4;44963;physiochemical_protein;numeric;10,3101;369,317;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
388;F5;44963;physiochemical_protein;numeric;319490,2166;5472011,408;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
389;F6;44963;physiochemical_protein;numeric;31,9704;598,408;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
390;F7;44963;physiochemical_protein;numeric;0;105948,17;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
391;F8;44963;physiochemical_protein;numeric;0;350;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
392;F9;44963;physiochemical_protein;numeric;15,228;55,3009;"**Data Description**

This is a data set of Physicochemical Properties of Protein Tertiary Structure. The data set is taken from CASP 5-9. There are 45730 decoys and size varying from 0 to 21 armstrong.

The goal of the dataset is to predict the size of the residue for a tertiary protein structure (a 3d protein structure). Once linked in the protein chain, an individual amino acid is called a residue. The target feature is root mean square error of the residue.

**Attribute Description**

1. *RMSD* - size of the residue
2. *F1* - total surface area
3. *F2* - non polar exposed area
4. *F3* - fractional area of exposed non polar residue
5. *F4* - fractional area of exposed non polar part of residue
6. *F5* - molecular mass weighted exposed area
7. *F6* - average deviation from standard exposed area of residue
8. *F7* - Euclidian distance
9. *F8* - secondary structure penalty
10. *F9* - Spacial Distribution constraints (N,K Value)";;;
393;Label;44270;turing_course_binary_data;numeric;0;1;Turing Binary Classification Dataset (with normalised features);;;
394;0;44270;turing_course_binary_data;numeric;-6,557421594;3,011165206;Turing Binary Classification Dataset (with normalised features);;;
395;1;44270;turing_course_binary_data;numeric;-2,372782976;2,855915688;Turing Binary Classification Dataset (with normalised features);;;
396;2;44270;turing_course_binary_data;numeric;-2,743479089;2,841569729;Turing Binary Classification Dataset (with normalised features);;;
397;3;44270;turing_course_binary_data;numeric;-3,046481894;3,362094633;Turing Binary Classification Dataset (with normalised features);;;
398;4;44270;turing_course_binary_data;numeric;-3,741947895;3,186604246;Turing Binary Classification Dataset (with normalised features);;;
399;5;44270;turing_course_binary_data;numeric;-2,732906096;2,776735341;Turing Binary Classification Dataset (with normalised features);;;
400;6;44270;turing_course_binary_data;numeric;-2,917721763;3,610133993;Turing Binary Classification Dataset (with normalised features);;;
401;7;44270;turing_course_binary_data;numeric;-3,653304369;4,504961104;Turing Binary Classification Dataset (with normalised features);;;
402;8;44270;turing_course_binary_data;numeric;-3,844105676;4,242365376;Turing Binary Classification Dataset (with normalised features);;;
403;9;44270;turing_course_binary_data;numeric;-3,828703126;4,251090843;Turing Binary Classification Dataset (with normalised features);;;
404;10;44270;turing_course_binary_data;numeric;-3,650459474;3,638061759;Turing Binary Classification Dataset (with normalised features);;;
405;11;44270;turing_course_binary_data;numeric;-3,780620312;4,589164322;Turing Binary Classification Dataset (with normalised features);;;
406;12;44270;turing_course_binary_data;numeric;-3,979526402;3,57194333;Turing Binary Classification Dataset (with normalised features);;;
407;13;44270;turing_course_binary_data;numeric;-3,286132269;4,077405164;Turing Binary Classification Dataset (with normalised features);;;
408;14;44270;turing_course_binary_data;numeric;-3,617918338;4,005752013;Turing Binary Classification Dataset (with normalised features);;;
409;15;44270;turing_course_binary_data;numeric;-3,673686504;4,252638968;Turing Binary Classification Dataset (with normalised features);;;
410;16;44270;turing_course_binary_data;numeric;-3,434010475;4,175181662;Turing Binary Classification Dataset (with normalised features);;;
411;17;44270;turing_course_binary_data;numeric;-3,798010779;4,242842645;Turing Binary Classification Dataset (with normalised features);;;
412;18;44270;turing_course_binary_data;numeric;-4,578622404;5,445014301;Turing Binary Classification Dataset (with normalised features);;;
413;19;44270;turing_course_binary_data;numeric;-3,903017364;3,484305959;Turing Binary Classification Dataset (with normalised features);;;
414;20;44270;turing_course_binary_data;numeric;-4,085853144;4,922359252;Turing Binary Classification Dataset (with normalised features);;;
415;21;44270;turing_course_binary_data;numeric;-3,50383497;3,845776402;Turing Binary Classification Dataset (with normalised features);;;
416;22;44270;turing_course_binary_data;numeric;-3,577675751;4,256484991;Turing Binary Classification Dataset (with normalised features);;;
417;23;44270;turing_course_binary_data;numeric;-3,329408248;3,679952669;Turing Binary Classification Dataset (with normalised features);;;
418;24;44270;turing_course_binary_data;numeric;-4,364380779;4,706710848;Turing Binary Classification Dataset (with normalised features);;;
419;25;44270;turing_course_binary_data;numeric;-3,712648533;4,231201037;Turing Binary Classification Dataset (with normalised features);;;
420;26;44270;turing_course_binary_data;numeric;-3,806062263;4,211679237;Turing Binary Classification Dataset (with normalised features);;;
421;27;44270;turing_course_binary_data;numeric;-3,998474416;3,851805443;Turing Binary Classification Dataset (with normalised features);;;
422;28;44270;turing_course_binary_data;numeric;-3,510997723;5,056180979;Turing Binary Classification Dataset (with normalised features);;;
423;29;44270;turing_course_binary_data;numeric;-4,076485179;3,759742357;Turing Binary Classification Dataset (with normalised features);;;
424;30;44270;turing_course_binary_data;numeric;-4,05088874;4,205652967;Turing Binary Classification Dataset (with normalised features);;;
425;31;44270;turing_course_binary_data;numeric;-3,747531385;4,667299681;Turing Binary Classification Dataset (with normalised features);;;
426;32;44270;turing_course_binary_data;numeric;-4,407333724;4,363621871;Turing Binary Classification Dataset (with normalised features);;;
427;33;44270;turing_course_binary_data;numeric;-3,848385751;3,883111332;Turing Binary Classification Dataset (with normalised features);;;
428;34;44270;turing_course_binary_data;numeric;-3,357139721;3,421823042;Turing Binary Classification Dataset (with normalised features);;;
429;35;44270;turing_course_binary_data;numeric;-3,706062399;4,409661718;Turing Binary Classification Dataset (with normalised features);;;
430;36;44270;turing_course_binary_data;numeric;-3,832276621;3,495382871;Turing Binary Classification Dataset (with normalised features);;;
431;37;44270;turing_course_binary_data;numeric;-4,11389775;4,472711652;Turing Binary Classification Dataset (with normalised features);;;
432;38;44270;turing_course_binary_data;numeric;-3,829966402;4,090507141;Turing Binary Classification Dataset (with normalised features);;;
433;39;44270;turing_course_binary_data;numeric;-3,776095194;3,537766923;Turing Binary Classification Dataset (with normalised features);;;
434;40;44270;turing_course_binary_data;numeric;-3,81352595;4,298351398;Turing Binary Classification Dataset (with normalised features);;;
435;41;44270;turing_course_binary_data;numeric;-4,006245929;4,079524273;Turing Binary Classification Dataset (with normalised features);;;
436;42;44270;turing_course_binary_data;numeric;-3,937434802;4,310215174;Turing Binary Classification Dataset (with normalised features);;;
437;43;44270;turing_course_binary_data;numeric;-3,559484094;4,817294204;Turing Binary Classification Dataset (with normalised features);;;
438;44;44270;turing_course_binary_data;numeric;-3,550758803;4,516028592;Turing Binary Classification Dataset (with normalised features);;;
439;45;44270;turing_course_binary_data;numeric;-3,781448658;4,057196949;Turing Binary Classification Dataset (with normalised features);;;
440;46;44270;turing_course_binary_data;numeric;-3,71405855;4,377162701;Turing Binary Classification Dataset (with normalised features);;;
441;47;44270;turing_course_binary_data;numeric;-3,388576086;3,536917952;Turing Binary Classification Dataset (with normalised features);;;
442;48;44270;turing_course_binary_data;numeric;-4,004913848;4,18470256;Turing Binary Classification Dataset (with normalised features);;;
443;49;44270;turing_course_binary_data;numeric;-3,526862745;3,954275831;Turing Binary Classification Dataset (with normalised features);;;
444;temperature;44195;Weather;numeric;64;85;The weather problem is a tiny dataset that we will use repeatedly to illustrate machine learning methods. Entirely fictitious, it supposedly concerns the conditions that are suitable for playing some unspecified game. In general, instances in a dataset are characterized by the values of features, or attributes, that measure different aspects of the instance. In this case there are four attributes: outlook, temperature, humidity, and windy. The outcome is whether to play or not.;;;
445;humidity;44195;Weather;numeric;65;96;The weather problem is a tiny dataset that we will use repeatedly to illustrate machine learning methods. Entirely fictitious, it supposedly concerns the conditions that are suitable for playing some unspecified game. In general, instances in a dataset are characterized by the values of features, or attributes, that measure different aspects of the instance. In this case there are four attributes: outlook, temperature, humidity, and windy. The outcome is whether to play or not.;;;
446;paramList;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
447;cyclomaticNum;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
448;loopNum;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
449;nestingDegree;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
450;SLOC;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
451;ALOC;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
452;localVars;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
453;localPtrVars;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
454;pointerArgs;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
455;callees;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
456;callers;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
457;height;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
458;conditions;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
459;cmps;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
460;jmps;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
461;ptrAssn;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
462;lable;44150;VulNoneVul;numeric;0;1;Vulnerability Dataset For Binary Classification;;;
463;AveragePrice;43927;avocado_sales;numeric;0,44;3,25;Historical data on avocado prices and sales volume in multiple US markets. For this version Date column is dropped and month and day information in kept.;;;
